{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from matplotlib import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us visualise the first 16 data points from the MNIST training data\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.imshow(mnist.train.images[i].reshape(28, 28), cmap='Greys_r')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = namedtuple('Model', 'x y_conv y_ keep_prob filt1_num filt1_size W_conv1 b_conv1 h_conv1 h_conv1_size')\n",
    "\n",
    "def create_model(input_len, output_len, filt1_num, filt1_size, filt2_num, filt2_size):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    x_ = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "    # Define the first convolution layer here\n",
    "    W_conv1 = weight_variable([filt1_size, filt1_size, 1, filt1_num])\n",
    "    b_conv1 = bias_variable([filt1_num])\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_, W_conv1, strides=[2, 2], padding='VALID') + b_conv1)\n",
    "\n",
    "    # Define the second convolution layer here\n",
    "    W_conv2 = weight_variable([filt2_size, filt2_size, filt1_num, filt2_num])\n",
    "    b_conv2 = bias_variable([filt2_num])\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_conv1, W_conv2, strides=[1, 1], padding='SAME') + b_conv2)\n",
    "\n",
    "    # Define maxpooling\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # All subsequent layers will be fully connected ignoring geometry so we'll flatten the layer\n",
    "    # Flatten the h_pool2_layer (as it has a multidimensiona shape)\n",
    "    flat_len = int(h_pool2.shape[1] * h_pool2.shape[2] * h_pool2.shape[3])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, flat_len])\n",
    "\n",
    "    # Define the first fully connected layer here\n",
    "    W_fc1 = weight_variable([flat_len, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Use dropout for this layer (should you wish)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, rate=1-keep_prob)\n",
    "\n",
    "    # The final fully connected layer\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    sess.run(tf.variables_initializer([W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2]))\n",
    "    \n",
    "    return Model(x, y_conv, y_, keep_prob, filt1_num, filt1_size, W_conv1, b_conv1, h_conv1, int(h_conv1.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compilation = namedtuple('Compilation', 'train_step accuracy')\n",
    "\n",
    "def compile_model(mod):\n",
    "    # We'll use the cross entropy loss function \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=mod.y_conv, labels=mod.y_))\n",
    "\n",
    "    # And classification accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(mod.y_conv, 1), tf.argmax(mod.y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # And the Adam optimiser\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "    \n",
    "    sess.run(tf.variables_initializer(optimizer.variables()))\n",
    "    \n",
    "    return Compilation(train_step, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a tf session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimisation algorithm\n",
    "\n",
    "def train_model(mod, comp, iters):\n",
    "    interval = iters // 10\n",
    "    for i in range(iters):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % interval == 0:\n",
    "            train_accuracy = sess.run(comp.accuracy, feed_dict={mod.x: batch[0], mod.y_: batch[1], mod.keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        sess.run(comp.train_step, feed_dict={mod.x: batch[0], mod.y_: batch[1], mod.keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a model\n",
    "\n",
    "def test_model(mod, comp):\n",
    "    acc = sess.run(comp.accuracy, feed_dict={mod.x: mnist.test.images, mod.y_: mnist.test.labels, mod.keep_prob: 1.0})\n",
    "    print ('Test accuracy: %g' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the filters in the first convolutional layer\n",
    "\n",
    "def visualise_filters(mod):\n",
    "    W = sess.run(mod.W_conv1)\n",
    "\n",
    "    num = mod.filt1_num\n",
    "    rows = int(num ** 0.5)\n",
    "    cols = (mod.filt1_num - 1) // rows + 1\n",
    "    fig = plt.figure()\n",
    "    for i in range(num):\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.imshow(W[:, :, 0, i], cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the activations of the first convolutional layer\n",
    "\n",
    "def visualise_activations(mod, filt_num, top_num):\n",
    "    H = sess.run(mod.h_conv1, feed_dict={mod.x: mnist.test.images})\n",
    "\n",
    "    res_size = mod.h_conv1_size\n",
    "    filt_size = mod.filt1_size\n",
    "    fig = plt.figure()\n",
    "    for i in range(filt_num):\n",
    "        curr_H = H[:, :, :, i]\n",
    "        top = np.argsort(curr_H, axis=None)\n",
    "        for j in range(top_num):\n",
    "            img_idx = top[-j] // (res_size ** 2)\n",
    "            x_coord = 2 * (top[-j] % (res_size ** 2) // res_size)\n",
    "            y_coord = 2 * (top[-j] % res_size)\n",
    "            show = mnist.train.images[img_idx].reshape(28, 28)[x_coord: x_coord + filt_size, y_coord: y_coord + filt_size]\n",
    "\n",
    "            ax = fig.add_subplot(filt_num, top_num, i * top_num + j + 1)\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "            ax.imshow(show, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = create_model(784, 10, 25, 12, 64, 5)\n",
    "comp = compile_model(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(mod, comp, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(mod, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_filters(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_activations(mod, 5, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = create_model(784, 10, 10, 5, 20, 5)\n",
    "comp2 = compile_model(mod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(mod2, comp2, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(mod2, comp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualise_filters(mod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualise_activations(mod2, 5, 12)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
