{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty model\n",
    "\n",
    "class EmptyModel:\n",
    "    def log_prob(self, data):\n",
    "        return 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'empty-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli model\n",
    "\n",
    "class BernoulliModel:\n",
    "    def __init__(self, data):\n",
    "        data = np.vectorize(int)(data)\n",
    "        n = len(data) + 2\n",
    "        zero_count = np.count_nonzero(data == 0) + 1\n",
    "        one_count = np.count_nonzero(data == 1) + 1\n",
    "        assert zero_count + one_count == n\n",
    "        self.zero_log_prob = np.log(zero_count) - np.log(n)\n",
    "        self.one_log_prob = np.log(one_count) - np.log(n)\n",
    "        def lookup_one(n):\n",
    "            n = int(n)\n",
    "            if n == 0: return self.zero_log_prob\n",
    "            elif n == 1: return self.one_log_prob\n",
    "            else: return 0\n",
    "        self.lookup = np.vectorize(lookup_one)\n",
    "        \n",
    "    def log_prob(self, data):\n",
    "        return self.lookup(data)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'bernoulli-model(' + str(np.exp(self.one_log_prob)) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinoulli model\n",
    "\n",
    "class MultinoulliModel:\n",
    "    def __init__(self, data, labels):\n",
    "        n = len(data) + len(labels)\n",
    "        curr_labels, counts = np.unique(data, return_counts=True)\n",
    "        log_probs = np.log(counts + 1) - np.log(n)\n",
    "        self.class_log_probs = dict(zip(curr_labels, log_probs))\n",
    "        for lab in labels:\n",
    "            if not lab in self.class_log_probs:\n",
    "                self.class_log_probs[lab] =  - np.log(n)\n",
    "        def lookup_one(lab):\n",
    "            if lab in self.class_log_probs: return self.class_log_probs[lab]\n",
    "            else: return 0\n",
    "        self.lookup = np.vectorize(lookup_one)\n",
    "        \n",
    "    def log_prob(self, data):\n",
    "        return self.lookup(data)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'multinoulli-model(' + str({k: np.exp(v) for (k, v) in sorted(self.class_log_probs.items())}) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gausian model\n",
    "\n",
    "class GausianModel:\n",
    "    def __init__(self, data):\n",
    "        data = np.vectorize(float)(data)\n",
    "        self.mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        self.var = max(std ** 2, 1e-6)\n",
    "        self.scale = - np.log(2 * np.pi * self.var) / 2\n",
    "        \n",
    "    def log_prob(self, data):\n",
    "        data = np.vectorize(float)(data)\n",
    "        return ( - (data - self.mean) ** 2 / (2 * self.var)) + self.scale\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'gaussian-model(' + str(self.mean) + ', ' + str(np.sqrt(self.var)) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier\n",
    "\n",
    "class NBC:\n",
    "    def __init__(self, feature_types, missing=[]):\n",
    "        self.feature_types = feature_types\n",
    "        self.missing = missing\n",
    "    \n",
    "    def get_feature_prob(self, feature, label):\n",
    "        model = models_by_class\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # set class probabilities\n",
    "        labels, counts = np.unique(y, return_counts=True)\n",
    "        log_probs = np.log(counts) - np.log(len(y))\n",
    "        self.class_log_probs = dict(zip(labels, log_probs))\n",
    "        self.labels = labels\n",
    "        missing = self.missing\n",
    "        \n",
    "        # split data by classes\n",
    "        X_by_class = dict()\n",
    "        for lab in labels:\n",
    "            indices = np.where(y == lab)[0]\n",
    "            X_by_class[lab] = np.take(X, indices, axis=0)\n",
    "        \n",
    "        # find cathegorical feature classes\n",
    "        num_features = len(self.feature_types)\n",
    "        feature_lables = dict()\n",
    "        for i in range(num_features):\n",
    "            if self.feature_types[i] == 'c':\n",
    "                data = X[:, i]\n",
    "                if len(missing) > i:\n",
    "                    ms = missing[i]\n",
    "                    data = data[data != ms]\n",
    "                feature_lables[i] = np.unique(data)\n",
    "        \n",
    "        # fit models\n",
    "        self.models = dict()\n",
    "        for lab in labels:\n",
    "            self.models[lab] = list()\n",
    "            for i in range(num_features):\n",
    "                data = X_by_class[lab][:, i]\n",
    "                if len(missing) > i:\n",
    "                    ms = missing[i]\n",
    "                    data = data[data != ms]\n",
    "                feature_type = self.feature_types[i]\n",
    "                if feature_type == 'n':\n",
    "                    model = EmptyModel()\n",
    "                elif feature_type == 'b':\n",
    "                    model = BernoulliModel(data)\n",
    "                elif feature_type == 'c':\n",
    "                    model = MultinoulliModel(data, feature_lables[i])\n",
    "                elif feature_type == 'r':\n",
    "                    model = GausianModel(data)\n",
    "                else:\n",
    "                    model = EmptyModel()\n",
    "                    raise Exception('Unknown feature type {}.'.format(feature_type))\n",
    "                self.models[lab].append(model)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n = len(self.feature_types)\n",
    "        def get_lab_log_prob(lab):  \n",
    "            models = self.models[lab]\n",
    "            def get_feature_log_prob(i):\n",
    "                return models[i].log_prob(X[:, i])\n",
    "            flps = np.array(list(map(get_feature_log_prob, np.arange(n))))\n",
    "            return np.sum(flps, axis=0) + self.class_log_probs[lab]\n",
    "        lab_log_probs = np.array(list(map(get_lab_log_prob, self.labels)))\n",
    "        lab_idxs = np.argmax(lab_log_probs, axis=0)\n",
    "        return self.labels[lab_idxs]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle a dataset\n",
    "\n",
    "def shuffle_dataset(X, y):\n",
    "    N, D = X.shape\n",
    "    Ntrain = int(0.8 * N)\n",
    "    shuffler = np.random.permutation(N)\n",
    "    Xtrain = X[shuffler[:Ntrain]]\n",
    "    ytrain = y[shuffler[:Ntrain]]\n",
    "    Xtest = X[shuffler[Ntrain:]]\n",
    "    ytest = y[shuffler[Ntrain:]]\n",
    "    return Ntrain, Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find learning curve\n",
    "\n",
    "intervals = 10\n",
    "\n",
    "def learning_curve_round(model, X, y):\n",
    "    Ntrain, Xtrain, ytrain, Xtest, ytest = shuffle_dataset(X, y)\n",
    "    result = list()\n",
    "    for i in range(intervals):\n",
    "        N = int(Ntrain * (i + 1) / intervals)\n",
    "        model.fit(Xtrain[:N], ytrain[:N])\n",
    "        yhat = model.predict(Xtest)\n",
    "        test_error = np.mean(yhat != ytest)\n",
    "        result.append(test_error)\n",
    "    return np.array(result)\n",
    "\n",
    "def learning_curve(model, X, y, iters=200):\n",
    "    result = list()\n",
    "    for i in range(iters):\n",
    "        result.append(learning_curve_round(model, X, y))\n",
    "    return np.mean(np.array(result), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curves(title, triples, iters=200):\n",
    "    xs = (np.arange(intervals) + 1) * 100 / intervals\n",
    "    for (model, X, y) in triples:\n",
    "        plt.plot(xs, learning_curve(model, X, y, iters))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder\n",
    "\n",
    "def one_hot_encode(X, encode_features, missing=[]):\n",
    "    n, m = X.shape\n",
    "    new_features = list()\n",
    "    features = list(X.transpose())\n",
    "    for i in range(m):\n",
    "        feature = features[i]\n",
    "        if encode_features[i] == 'n':\n",
    "            new_features.append(feature)\n",
    "        else:\n",
    "            labels = np.unique(feature)\n",
    "            if len(missing) > i:\n",
    "                labels = labels[labels != missing[i]]\n",
    "            for lab in labels:\n",
    "                new_features.append(np.equal(feature, np.full(n, lab)))\n",
    "    return np.array(new_features).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrc = LogisticRegression(C=5.0, multi_class='ovr', solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def load_iris_dataset():\n",
    "    iris = load_iris()\n",
    "    return iris['data'], iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load voting dataset\n",
    "\n",
    "def load_voting_dataset():\n",
    "    return cp.load(open('voting.pickle', 'rb'))\n",
    "\n",
    "def load_full_voting_dataset():\n",
    "    return cp.load(open('voting_full.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adult dataset\n",
    "\n",
    "def load_adult_dataset():\n",
    "    return cp.load(open('adult.pickle', 'rb'))\n",
    "\n",
    "def load_full_adult_dataset():\n",
    "    return cp.load(open('adult_full.pickle', 'rb'))\n",
    "\n",
    "def load_ugly_adult_dataset():\n",
    "    return cp.load(open('adult_ugly.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris_dataset()\n",
    "nbc = NBC(feature_types='rrrr')\n",
    "plot_learning_curves('Iris', [(nbc, X, y), (lrc, X, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_voting_dataset()\n",
    "nbc = NBC(feature_types='bbbbbbbbbbbbbbbb')\n",
    "plot_learning_curves('Voting', [(nbc, X, y), (lrc, X, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_full_voting_dataset()\n",
    "X_ohe = one_hot_encode(X, 'yyyyyyyyyyyyyyyy',  missing=np.repeat(2, 16))\n",
    "nbc_i = NBC(feature_types='bbbbbbbbbbbbbbbb', missing=np.repeat(2, 16))\n",
    "nbc_u = NBC(feature_types='cccccccccccccccc')\n",
    "plot_learning_curves('Ignore vs use vs lr - full voting', [(nbc_i, X, y), (nbc_u, X, y), (lrc, X_ohe, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X , y = load_adult_dataset()\n",
    "X = X[:1000]\n",
    "y = y[:1000]\n",
    "X_ohe = one_hot_encode(X, 'nynynyyyyynnny')\n",
    "nbc = NBC(feature_types='rcrcrcccccrrrc')\n",
    "plot_learning_curves('Adult', [(nbc, X, y), (lrc, X_ohe, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , y = load_full_adult_dataset()\n",
    "X = X[:1000]\n",
    "y = y[:1000]\n",
    "X_ohe = one_hot_encode(X, 'nynynyyyyynnny', missing=np.repeat(-1, 16))\n",
    "nbc_i = NBC(feature_types='rcrcrcccccrrrc', missing=np.repeat(-1, 16))\n",
    "nbc_u = NBC(feature_types='rcrcrcccccrrrc')\n",
    "plot_learning_curves('Ignore vs use vs lr - full adult', [(nbc_i, X, y), (nbc_u, X, y), (lrc, X_ohe, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c , y_c = load_full_adult_dataset()\n",
    "X_c = X_c[:1000]\n",
    "y_c = y_c[:1000]\n",
    "X_u, y_u = load_ugly_adult_dataset()\n",
    "X_u = X_u[:1000]\n",
    "y_u = y_u[:1000]\n",
    "nbc_c = NBC(feature_types='rcrcrcccccrrrc', missing=np.repeat(-1, 16))\n",
    "nbc_u = NBC(feature_types='rcrcrcccccrrrc', missing=np.repeat('?', 16))\n",
    "plot_learning_curves('Clean vs ugly - full adult', [(nbc_c, X_c, y_c), (nbc_u, X_u, y_u)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u, y_u = load_ugly_adult_dataset()\n",
    "print(X_u[0], y_u[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
